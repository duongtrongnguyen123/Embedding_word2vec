```
Embedding_word2vec/
â”œâ”€ data/                                  # Artifacts & small fixtures
â”‚  â”œâ”€ embed_in.pt                         # Learned input vectors  (|V| Ã— d)
â”‚  â”œâ”€ embed_out.pt                        # Learned output vectors (|V| Ã— d)
â”‚  â”œâ”€ train_starts.bin                    # Start offsets of train docs/spans
â”‚  â”œâ”€ valid_starts.bin                    # Start offsets of valid docs/spans
â”‚  â”œâ”€ vocab.pt                            # {old id -> new id, token -> id, id -> token, freq, keep probs, ...}
â”‚  â””â”€ test_embed/                         # toy set for smoke test
â”‚     â”œâ”€ n_corpus.bin                     # 
â”‚     â”œâ”€ o_corpus.bin
â”‚     â””â”€ vocab.pt
â”‚
â”œâ”€ srcs/
â”‚  â”œâ”€ data_pipeline/                     # Build vocab â†’ encode corpus â†’ ID pipeline
â”‚  â”‚  â”œâ”€ _count_fast.pyx                 # Cython: token/bigram counter (pass 1)
â”‚  â”‚  â”œâ”€ _encode_corpus.pyx              # Cython: encode to token IDs (pass 2)
â”‚  â”‚  â”œâ”€ count_tokens.py                 # func wrapper for _count_fast
â”‚  â”‚  â”œâ”€ encode_corpus.py                # func wrapper for _encode_corpus
â”‚  â”‚  â”œâ”€ data_pipe_ids.py                # Iterable over (center_id, context_id)
â”‚  â”‚  â”œâ”€ review_dataset_iter.py          # Iterator for Reviews_and_TV datasets
â”‚  â”‚  â””â”€ setup.py                        # Build Cython extensions
â”‚  â”‚
â”‚  â”œâ”€ embedding/                         # Training
â”‚  â”‚  â””â”€ embedding_ids.py                # SGNS using ID pipeline 
â”‚  â”‚
â”‚  â”œâ”€ notebook/                          # Experiments
â”‚  â”‚  â”œâ”€ train_reviews_ids.ipynb         # Kaggle reviews_and_TV
â”‚  â”‚
â”‚  â””â”€ test/                              # Verification & speed
â”‚     â”œâ”€ semantic.py                     # Simple semantic similarity checks
â”‚     â”œâ”€ speedtest.py                    # mul+sum vs bmm vs einsum
â”‚     â”œâ”€ test_encode.py                  # Encode-corpus correctness
â”‚     â””â”€ test_fast_count.py              # Counter correctness
â”‚
â”œâ”€ README.md                              # You are here
â””â”€ LICENSE
```


## Approach

Implement Skip-Gram with Negative Sampling (SGNS) from scratch, with a custom preprocessing pipeline optimized for large corpora.
The pipeline is two-pass:

1. **Count tokens & candidate bigrams**
   - Collect unigram counts
   - Collect top-K high-frequency token pairs

2. **Build final vocab + re-encode corpus**
   - Apply `min_count` threshold
   - Drop tokens not in final vocab
   - Re-encode corpus into integer ID streams

Additionally apply POS-aware bigram merging, which helps preserve meaningful multi-word units:
- NOUN + NOUN
- NEGATION + ADJ/ADV (e.g., `not_good`)
- VERB + PARTICLE (e.g., `pick_up`)

Counts for merged pairs are lightly smoothed before integration.

To improve signal quality, subsampling keep-probabilities are computed with POS-aware masks, reducing noise while preserving sentiment-bearing adjectives/adverbs.

Training is implemented in PyTorch using efficient ID-based sampling.

# Key Improvements over Vanilla Word2Vec

- Fast preprocessing via Cython (`_count_fast.pyx`, `_encode_corpus.pyx`)
- POS-aware phrase merging (e.g., `not_good`, `good_movie`, `pick_up`)
- POS-conditioned subsampling keeps important token classes (ADJ/ADV)
- Clean ID pipeline ensures no stray OOV during training
- Simple semantic evaluation


# ðŸ“Š Evaluation

Nearest neighbors
```
happy       â†’ camper, satisfied, pleased
good        â†’ ol'_days, documentry, not_surprising
bad         â†’ guys, horrible, ruined
excellent   â†’ giovanni, suberb, stephan
outstanding â†’ phenomenal, superb, exceptional
masterpiece â†’ finest, must-see, assuredly
awful       â†’ lousy, dreadful, horrible
terrible    â†’ horrible, lousy, ruined
boring      â†’ not_help, predictable, predicable
cringe      â†’ wince, cringeworthy, sophmoric
```

Analogy
```
king - man + woman â†’ queen, mistress, prince
```

2D Embedding Visualization
<p align="center">
  <img src="results/embedding_pca.png" width="700">
</p>


# ðŸš€ How to Run 
```
# from project root
cd srcs/data_pipeline
python setup.py build_ext --inplace
cd ../../

python srcs/data_pipeline/count_tokens.py
python srcs/data_pipeline/encode_corpus.py
python -m srcs.embedding.embedding_ids
```

# Default parameter
```
top_k = 10000        (pick top_k highest freq count)
min_pair_count = 6
min_count = 25
t = 6e-6
```


